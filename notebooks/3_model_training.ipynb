{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1Ehr9UUvJWAYaUXjnsAW5RZ4RXyOUnqbL","authorship_tag":"ABX9TyOIS/D0ZCe6ZYyNcp3QtMbt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"34249ce97436464da4e64cf4345fbebd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eddfb58d506743d89d656f6b76e54dfd","IPY_MODEL_8a99130b8e6b4ac1b7384fcd48f023c5","IPY_MODEL_121806ed2c0449c0816ba45a1d8be067"],"layout":"IPY_MODEL_8236ed84b848467c9ef0aaaabcb9fa63"}},"eddfb58d506743d89d656f6b76e54dfd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be97417aa87a4170aa72f2b20a13161e","placeholder":"​","style":"IPY_MODEL_fadc928bcf464543826eb323476f1bae","value":"tokenizer_config.json: 100%"}},"8a99130b8e6b4ac1b7384fcd48f023c5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_59099e5c988f45bc8de8298fccb22275","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4d2c8ef897c9473089388104bead5c37","value":25}},"121806ed2c0449c0816ba45a1d8be067":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d073ce091f7c41dbaced549ef264e734","placeholder":"​","style":"IPY_MODEL_1750c4a8c86e40558451f0cba74e8fe7","value":" 25.0/25.0 [00:00&lt;00:00, 691B/s]"}},"8236ed84b848467c9ef0aaaabcb9fa63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be97417aa87a4170aa72f2b20a13161e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fadc928bcf464543826eb323476f1bae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"59099e5c988f45bc8de8298fccb22275":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d2c8ef897c9473089388104bead5c37":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d073ce091f7c41dbaced549ef264e734":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1750c4a8c86e40558451f0cba74e8fe7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"42fc534a6f95484a91ae42c6a093275d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a338a718d47e48ed8321404c612cfec1","IPY_MODEL_189be5dc6dd54fd4bc0a7df10d9cdf42","IPY_MODEL_ba8facd277a94fe0a855255ffefcba7f"],"layout":"IPY_MODEL_9cabe07689684ecba18180c5371e4e0f"}},"a338a718d47e48ed8321404c612cfec1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b0002466291430a95e235468e31f1ee","placeholder":"​","style":"IPY_MODEL_6ca24de5ebe84aa88406c8e88a98d2dd","value":"config.json: 100%"}},"189be5dc6dd54fd4bc0a7df10d9cdf42":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e44e6fb6b494a4fa6c48942cfb9a8c2","max":615,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7564dedec8224be9b249a7f5be2714d9","value":615}},"ba8facd277a94fe0a855255ffefcba7f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_00e0b715a3e84a0ea161bd02877d51e8","placeholder":"​","style":"IPY_MODEL_0763cc262e164c7c83abdd4ea99edf10","value":" 615/615 [00:00&lt;00:00, 27.9kB/s]"}},"9cabe07689684ecba18180c5371e4e0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b0002466291430a95e235468e31f1ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ca24de5ebe84aa88406c8e88a98d2dd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e44e6fb6b494a4fa6c48942cfb9a8c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7564dedec8224be9b249a7f5be2714d9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"00e0b715a3e84a0ea161bd02877d51e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0763cc262e164c7c83abdd4ea99edf10":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"42566fe70bd04ce4aaf09ffd4c5dab64":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4bfe4299468d4674ac0c9ca18ac83d44","IPY_MODEL_174c08bc968a46e68853a0cb1a66ba2a","IPY_MODEL_2329eb32452448ad9b3c6e70e17cdc37"],"layout":"IPY_MODEL_ac2ffdb568dc44b0884c8150e810cb3a"}},"4bfe4299468d4674ac0c9ca18ac83d44":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12e2eaed0ebb4e75bfc78443c2119b45","placeholder":"​","style":"IPY_MODEL_dc7d63e6f7c5457c8c644e7b5b06bbd4","value":"sentencepiece.bpe.model: 100%"}},"174c08bc968a46e68853a0cb1a66ba2a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4e2f6d0163d429bac412c79549f7b29","max":5069051,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e303960e7403423fb88c3c5a54ed6346","value":5069051}},"2329eb32452448ad9b3c6e70e17cdc37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4bd449169bc4274aec034e71b78bfb4","placeholder":"​","style":"IPY_MODEL_38d834f7887b4a179ba341d2f78daa20","value":" 5.07M/5.07M [00:00&lt;00:00, 17.1MB/s]"}},"ac2ffdb568dc44b0884c8150e810cb3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12e2eaed0ebb4e75bfc78443c2119b45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc7d63e6f7c5457c8c644e7b5b06bbd4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a4e2f6d0163d429bac412c79549f7b29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e303960e7403423fb88c3c5a54ed6346":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b4bd449169bc4274aec034e71b78bfb4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38d834f7887b4a179ba341d2f78daa20":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae4d7f092de642a2a790a702f2246a5e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_711cd67dbca24b939322a782c8c8bcf8","IPY_MODEL_4be52b238edc4ef8bcc479539886cb7e","IPY_MODEL_0a2515ab74b84a4d87524cd07b551b08"],"layout":"IPY_MODEL_5638b29ee78c4007a9e45fb970b813d3"}},"711cd67dbca24b939322a782c8c8bcf8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_54a65194e8e44b5a802d844bffa8bff5","placeholder":"​","style":"IPY_MODEL_85382ba3d2ef495cb2f014e13192a480","value":"tokenizer.json: 100%"}},"4be52b238edc4ef8bcc479539886cb7e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5ffc6e4cd9c4dd8b83d323d5f397721","max":9096718,"min":0,"orientation":"horizontal","style":"IPY_MODEL_26bc25c1c1d04c5f898bdc1f49ecf883","value":9096718}},"0a2515ab74b84a4d87524cd07b551b08":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d4f9a0067ec48e4b6e0ae9705b346b2","placeholder":"​","style":"IPY_MODEL_c185fb590db14931ba2d477518ac77dc","value":" 9.10M/9.10M [00:00&lt;00:00, 23.5MB/s]"}},"5638b29ee78c4007a9e45fb970b813d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54a65194e8e44b5a802d844bffa8bff5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85382ba3d2ef495cb2f014e13192a480":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e5ffc6e4cd9c4dd8b83d323d5f397721":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26bc25c1c1d04c5f898bdc1f49ecf883":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5d4f9a0067ec48e4b6e0ae9705b346b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c185fb590db14931ba2d477518ac77dc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d3b4dd14e70746eea266534bd4296e00":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3e16832470e949d290d7188ab37419c3","IPY_MODEL_b71d04d93ffd4e6cb8057b681346ec5a","IPY_MODEL_38257e4577ce40fbad780895b0872945"],"layout":"IPY_MODEL_f2e96bd81b314762a1e6ac966db4da79"}},"3e16832470e949d290d7188ab37419c3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d437c68fc01242ad8487c4b2cc195b54","placeholder":"​","style":"IPY_MODEL_28caeb29017b4bc8b4e676f5bb814174","value":"model.safetensors: 100%"}},"b71d04d93ffd4e6cb8057b681346ec5a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a033d97f92942598f007e389295862c","max":1115567652,"min":0,"orientation":"horizontal","style":"IPY_MODEL_96706bc0d57a41ac9ecbe03d91bce237","value":1115567652}},"38257e4577ce40fbad780895b0872945":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f0862e3dd394a6ca1be02554e45d872","placeholder":"​","style":"IPY_MODEL_f032d1202ff84946a5032896e54ae5f8","value":" 1.12G/1.12G [00:28&lt;00:00, 57.2MB/s]"}},"f2e96bd81b314762a1e6ac966db4da79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d437c68fc01242ad8487c4b2cc195b54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28caeb29017b4bc8b4e676f5bb814174":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a033d97f92942598f007e389295862c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96706bc0d57a41ac9ecbe03d91bce237":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1f0862e3dd394a6ca1be02554e45d872":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f032d1202ff84946a5032896e54ae5f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1fc1424a14c34b65ab73794d6325e244":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_93162888415740ad876bd8734848a436","IPY_MODEL_c52ccbee31714e5eaa298a752beeed2d","IPY_MODEL_a832559db1dd444f941ffe124ed27571"],"layout":"IPY_MODEL_5ef04b933ae3450d8f8bf04b1f03d258"}},"93162888415740ad876bd8734848a436":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_becd62afcddd4e6f9086bb216bebdf8f","placeholder":"​","style":"IPY_MODEL_588a99052bc348549ff7de6a475280c1","value":"Map: 100%"}},"c52ccbee31714e5eaa298a752beeed2d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_667b83707d734e058c5124a3c76bc5b3","max":8,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cc9e8a32cd4d4b56b2c5433aafb397bd","value":8}},"a832559db1dd444f941ffe124ed27571":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a2651749f584eb69a8c4e9f2985b0a2","placeholder":"​","style":"IPY_MODEL_2e4c0b77f8294a89bcf03031539ceb9f","value":" 8/8 [00:00&lt;00:00, 95.43 examples/s]"}},"5ef04b933ae3450d8f8bf04b1f03d258":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"becd62afcddd4e6f9086bb216bebdf8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"588a99052bc348549ff7de6a475280c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"667b83707d734e058c5124a3c76bc5b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc9e8a32cd4d4b56b2c5433aafb397bd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5a2651749f584eb69a8c4e9f2985b0a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e4c0b77f8294a89bcf03031539ceb9f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33593c8094ce4c088168b4b2fa847b4f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6e1c1de918ad4d3389214495f5501075","IPY_MODEL_87a6bb586dd84d00bd17b920e485c9ce","IPY_MODEL_889ac13ae9f943a69b9962459761f8ba"],"layout":"IPY_MODEL_39afdeccabd24e6189eba0b145b6f717"}},"6e1c1de918ad4d3389214495f5501075":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a0f08ad336e4809b7e23cc1e112cada","placeholder":"​","style":"IPY_MODEL_c921493a22b14c5daf687a885e79e7d0","value":"Map: 100%"}},"87a6bb586dd84d00bd17b920e485c9ce":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_492225a155cc4ce5b73700ebc9d20c96","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_946245cd688242a29d5ecfb9c918a150","value":2}},"889ac13ae9f943a69b9962459761f8ba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_037116e37c784910893f2fc46a733c8f","placeholder":"​","style":"IPY_MODEL_840331779cae40fb9182f661d6918a35","value":" 2/2 [00:00&lt;00:00, 19.78 examples/s]"}},"39afdeccabd24e6189eba0b145b6f717":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a0f08ad336e4809b7e23cc1e112cada":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c921493a22b14c5daf687a885e79e7d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"492225a155cc4ce5b73700ebc9d20c96":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"946245cd688242a29d5ecfb9c918a150":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"037116e37c784910893f2fc46a733c8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"840331779cae40fb9182f661d6918a35":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7292d9099d0340edae3a78d3b40a22f8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_069433a85e0046e198e159c1722bf5f1","IPY_MODEL_11c35adfc5d242a6b127782bafaeb3c7","IPY_MODEL_029e84deb8bf42a9b4d6f14eccf6407e"],"layout":"IPY_MODEL_03c5d5f0fb1a4c27966f87799670edee"}},"069433a85e0046e198e159c1722bf5f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfcc1be95ef4437daa79f7be4b9a3f59","placeholder":"​","style":"IPY_MODEL_2202e8504755480eb03658e5e1c0a1a4","value":"Map: 100%"}},"11c35adfc5d242a6b127782bafaeb3c7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_879d50e59b8f45eb8b63561c835cb980","max":8,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f78c7da06d284d859bea4e4fefb30b06","value":8}},"029e84deb8bf42a9b4d6f14eccf6407e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5b48882f2bf405987bf834b2580809a","placeholder":"​","style":"IPY_MODEL_69efa8558cd44ee9b51c5aeb754c791b","value":" 8/8 [00:00&lt;00:00, 145.14 examples/s]"}},"03c5d5f0fb1a4c27966f87799670edee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfcc1be95ef4437daa79f7be4b9a3f59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2202e8504755480eb03658e5e1c0a1a4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"879d50e59b8f45eb8b63561c835cb980":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f78c7da06d284d859bea4e4fefb30b06":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d5b48882f2bf405987bf834b2580809a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69efa8558cd44ee9b51c5aeb754c791b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7c9e448f6802407dbe13234ac7c9bab7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_707d95fac8df4c7599821840e6f25b24","IPY_MODEL_ef376d98e28d48a395ad40898f196bb5","IPY_MODEL_470550f583bc44469f357b63008651f7"],"layout":"IPY_MODEL_cc1d977c0c0f4a5aa96661f419966d90"}},"707d95fac8df4c7599821840e6f25b24":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_34ac351d9cbb44f4990f8cf1c4517c3d","placeholder":"​","style":"IPY_MODEL_39adb4dabd8646edbd39f02668c13833","value":"Map: 100%"}},"ef376d98e28d48a395ad40898f196bb5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_15540f6473c149dd8b33d9704dd715a8","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3cac139250724422b6a6f5dcadcb9caa","value":2}},"470550f583bc44469f357b63008651f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2cfe49185366491893457305810662ac","placeholder":"​","style":"IPY_MODEL_0dcedba7fcaf478184dae6bf84dc99a3","value":" 2/2 [00:00&lt;00:00, 58.80 examples/s]"}},"cc1d977c0c0f4a5aa96661f419966d90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34ac351d9cbb44f4990f8cf1c4517c3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39adb4dabd8646edbd39f02668c13833":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"15540f6473c149dd8b33d9704dd715a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3cac139250724422b6a6f5dcadcb9caa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2cfe49185366491893457305810662ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0dcedba7fcaf478184dae6bf84dc99a3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["#Fine-Tuning an NER Model for Amharic Text"],"metadata":{"id":"zDAALsNO0hlf"}},{"cell_type":"markdown","source":["##Setup Environment\n","\n","Install Libraries:\n"],"metadata":{"id":"MubMupdHtyXE"}},{"cell_type":"code","source":["!pip install transformers datasets seqeval -q"],"metadata":{"id":"mn-y1yujt_mA","executionInfo":{"status":"ok","timestamp":1750963103268,"user_tz":-180,"elapsed":26000,"user":{"displayName":"ImEl","userId":"00735694531887898070"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"757b3b60-35cd-450a-828f-75f3e1bf6d9d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"markdown","source":["##Load Dataset"],"metadata":{"id":"Rcm9VxPl0wgc"}},{"cell_type":"code","source":["from datasets import load_from_disk\n","\n","dataset = load_from_disk(\"/content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/data/hf_ner_dataset\")\n","dataset = dataset.train_test_split(test_size=0.2, seed=42)\n","dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wa31Rdue02SL","executionInfo":{"status":"ok","timestamp":1750963111232,"user_tz":-180,"elapsed":7947,"user":{"displayName":"ImEl","userId":"00735694531887898070"}},"outputId":"3d248eac-8cae-4dc4-f7c2-7502116a52c4"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['tokens', 'ner_tags'],\n","        num_rows: 8\n","    })\n","    test: Dataset({\n","        features: ['tokens', 'ner_tags'],\n","        num_rows: 2\n","    })\n","})"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["##Load the Tokenizer and Model\n","**I use the xlm-roberta.**\n","This is a public multilingual NER model trained on low-resource languages including African ones."],"metadata":{"id":"qkyercfh0t2m"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForTokenClassification\n","\n","model_checkpoint = \"xlm-roberta-base\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336,"referenced_widgets":["34249ce97436464da4e64cf4345fbebd","eddfb58d506743d89d656f6b76e54dfd","8a99130b8e6b4ac1b7384fcd48f023c5","121806ed2c0449c0816ba45a1d8be067","8236ed84b848467c9ef0aaaabcb9fa63","be97417aa87a4170aa72f2b20a13161e","fadc928bcf464543826eb323476f1bae","59099e5c988f45bc8de8298fccb22275","4d2c8ef897c9473089388104bead5c37","d073ce091f7c41dbaced549ef264e734","1750c4a8c86e40558451f0cba74e8fe7","42fc534a6f95484a91ae42c6a093275d","a338a718d47e48ed8321404c612cfec1","189be5dc6dd54fd4bc0a7df10d9cdf42","ba8facd277a94fe0a855255ffefcba7f","9cabe07689684ecba18180c5371e4e0f","1b0002466291430a95e235468e31f1ee","6ca24de5ebe84aa88406c8e88a98d2dd","4e44e6fb6b494a4fa6c48942cfb9a8c2","7564dedec8224be9b249a7f5be2714d9","00e0b715a3e84a0ea161bd02877d51e8","0763cc262e164c7c83abdd4ea99edf10","42566fe70bd04ce4aaf09ffd4c5dab64","4bfe4299468d4674ac0c9ca18ac83d44","174c08bc968a46e68853a0cb1a66ba2a","2329eb32452448ad9b3c6e70e17cdc37","ac2ffdb568dc44b0884c8150e810cb3a","12e2eaed0ebb4e75bfc78443c2119b45","dc7d63e6f7c5457c8c644e7b5b06bbd4","a4e2f6d0163d429bac412c79549f7b29","e303960e7403423fb88c3c5a54ed6346","b4bd449169bc4274aec034e71b78bfb4","38d834f7887b4a179ba341d2f78daa20","ae4d7f092de642a2a790a702f2246a5e","711cd67dbca24b939322a782c8c8bcf8","4be52b238edc4ef8bcc479539886cb7e","0a2515ab74b84a4d87524cd07b551b08","5638b29ee78c4007a9e45fb970b813d3","54a65194e8e44b5a802d844bffa8bff5","85382ba3d2ef495cb2f014e13192a480","e5ffc6e4cd9c4dd8b83d323d5f397721","26bc25c1c1d04c5f898bdc1f49ecf883","5d4f9a0067ec48e4b6e0ae9705b346b2","c185fb590db14931ba2d477518ac77dc","d3b4dd14e70746eea266534bd4296e00","3e16832470e949d290d7188ab37419c3","b71d04d93ffd4e6cb8057b681346ec5a","38257e4577ce40fbad780895b0872945","f2e96bd81b314762a1e6ac966db4da79","d437c68fc01242ad8487c4b2cc195b54","28caeb29017b4bc8b4e676f5bb814174","0a033d97f92942598f007e389295862c","96706bc0d57a41ac9ecbe03d91bce237","1f0862e3dd394a6ca1be02554e45d872","f032d1202ff84946a5032896e54ae5f8"]},"id":"NfydArux3tjB","executionInfo":{"status":"ok","timestamp":1750963199433,"user_tz":-180,"elapsed":69101,"user":{"displayName":"ImEl","userId":"00735694531887898070"}},"outputId":"521195d9-db73-4ca5-ea5c-0fc88704cb3a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34249ce97436464da4e64cf4345fbebd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42fc534a6f95484a91ae42c6a093275d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42566fe70bd04ce4aaf09ffd4c5dab64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae4d7f092de642a2a790a702f2246a5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3b4dd14e70746eea266534bd4296e00"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","source":["##Preprocess Labels for Token Classification"],"metadata":{"id":"jmhzkLMs33k6"}},{"cell_type":"code","source":["label_list = list(set(label for row in dataset['train']['ner_tags'] for label in row))\n","label_list.sort()\n","label_to_id = {l: i for i, l in enumerate(label_list)}\n","id_to_label = {i: l for l, i in label_to_id.items()}\n","\n","def encode_tags(example):\n","    return {'labels': [label_to_id[tag] for tag in example['ner_tags']]}\n","\n","dataset = dataset.map(encode_tags)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["1fc1424a14c34b65ab73794d6325e244","93162888415740ad876bd8734848a436","c52ccbee31714e5eaa298a752beeed2d","a832559db1dd444f941ffe124ed27571","5ef04b933ae3450d8f8bf04b1f03d258","becd62afcddd4e6f9086bb216bebdf8f","588a99052bc348549ff7de6a475280c1","667b83707d734e058c5124a3c76bc5b3","cc9e8a32cd4d4b56b2c5433aafb397bd","5a2651749f584eb69a8c4e9f2985b0a2","2e4c0b77f8294a89bcf03031539ceb9f","33593c8094ce4c088168b4b2fa847b4f","6e1c1de918ad4d3389214495f5501075","87a6bb586dd84d00bd17b920e485c9ce","889ac13ae9f943a69b9962459761f8ba","39afdeccabd24e6189eba0b145b6f717","5a0f08ad336e4809b7e23cc1e112cada","c921493a22b14c5daf687a885e79e7d0","492225a155cc4ce5b73700ebc9d20c96","946245cd688242a29d5ecfb9c918a150","037116e37c784910893f2fc46a733c8f","840331779cae40fb9182f661d6918a35"]},"id":"a-9orX9-4PB7","executionInfo":{"status":"ok","timestamp":1750963202799,"user_tz":-180,"elapsed":301,"user":{"displayName":"ImEl","userId":"00735694531887898070"}},"outputId":"60045f6b-3b98-4c79-eb6a-cf0865d03bff"},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/8 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fc1424a14c34b65ab73794d6325e244"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/2 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33593c8094ce4c088168b4b2fa847b4f"}},"metadata":{}}]},{"cell_type":"code","source":["dataset[\"train\"][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QbUHrH3b62ru","executionInfo":{"status":"ok","timestamp":1750963208511,"user_tz":-180,"elapsed":153,"user":{"displayName":"ImEl","userId":"00735694531887898070"}},"outputId":"a10e457b-6813-4a7d-fbbb-6011728301ad"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'tokens': ['Threelayer',\n","  'Baby',\n","  'Milk',\n","  'Powder',\n","  'Container',\n","  '__',\n","  'High',\n","  'Quality',\n","  '__',\n","  '__',\n","  'Three',\n","  'Layer',\n","  'NoSpill',\n","  'Baby',\n","  'Feeding',\n","  'Milk',\n","  'Powder',\n","  'Food',\n","  'Dispenser',\n","  'A',\n","  'perfect',\n","  'storage',\n","  'for',\n","  'travel',\n","  'or',\n","  'home',\n","  'use__',\n","  '__እናት',\n","  'ልጇን',\n","  'ይዛ',\n","  'የተለያየ',\n","  'ቦታ',\n","  'ስትንቀሳቀስ',\n","  'የዱቄት',\n","  'ወተት',\n","  'የመሳሰሉትን',\n","  'አስፈላጊ',\n","  'የልጆች',\n","  'ምግብ',\n","  'ይዞ',\n","  'ለመንቀሳቀስ',\n","  'የሚረዳ',\n","  '3',\n","  'ፓርቲሽን',\n","  'ያለው',\n","  'አሪፍ',\n","  'ኮንቴነር__',\n","  'ዋጋ፦',\n","  '500ብር',\n","  'ውስን',\n","  'ፍሬ',\n","  'ነው',\n","  'ያለው',\n","  'አድራሻ',\n","  'መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ',\n","  'ቢሮ',\n","  'ቁ',\n","  'S05S06',\n","  '0902660722',\n","  '0928460606',\n","  'በTelegram',\n","  'ለማዘዝ',\n","  'ይጠቀሙ',\n","  'ለተጨማሪ',\n","  'ማብራሪያ',\n","  'የቴሌግራም',\n","  'ገፃችን'],\n"," 'ner_tags': ['B-Product',\n","  'I-Product',\n","  'I-Product',\n","  'I-Product',\n","  'I-Product',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'B-Product',\n","  'I-Product',\n","  'I-Product',\n","  'I-Product',\n","  'I-Product',\n","  'I-Product',\n","  'I-Product',\n","  'I-Product',\n","  'I-Product',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'B-Product',\n","  'I-Product',\n","  'O',\n","  'O',\n","  'O',\n","  'B-Product',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'B-Product',\n","  'O',\n","  'B-PRICE',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'B-LOC',\n","  'O',\n","  'O',\n","  'I-LOC',\n","  'B-CONTACT',\n","  'B-CONTACT',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O'],\n"," 'labels': [3,\n","  5,\n","  5,\n","  5,\n","  5,\n","  6,\n","  6,\n","  6,\n","  6,\n","  6,\n","  3,\n","  5,\n","  5,\n","  5,\n","  5,\n","  5,\n","  5,\n","  5,\n","  5,\n","  6,\n","  6,\n","  6,\n","  6,\n","  6,\n","  6,\n","  6,\n","  6,\n","  6,\n","  6,\n","  6,\n","  6,\n","  6,\n","  6,\n","  3,\n","  5,\n","  6,\n","  6,\n","  6,\n","  3,\n","  6,\n","  6,\n","  6,\n","  6,\n","  6,\n","  6,\n","  6,\n","  3,\n","  6,\n","  2,\n","  6,\n","  6,\n","  6,\n","  6,\n","  6,\n","  1,\n","  6,\n","  6,\n","  4,\n","  0,\n","  0,\n","  6,\n","  6,\n","  6,\n","  6,\n","  6,\n","  6,\n","  6]}"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["!pip install -U transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fGmOsmqIXHtF","executionInfo":{"status":"ok","timestamp":1750876589042,"user_tz":-180,"elapsed":12258,"user":{"displayName":"ImEl","userId":"00735694531887898070"}},"outputId":"c918765c-d5b0-4d19-afc0-f7a607bca152"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n"]}]},{"cell_type":"code","source":["import transformers\n","print(transformers.__version__)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8igLnjwvGxwr","executionInfo":{"status":"ok","timestamp":1750963246758,"user_tz":-180,"elapsed":34,"user":{"displayName":"ImEl","userId":"00735694531887898070"}},"outputId":"440f0d20-417e-4d9e-d7f4-01ada30de10c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["4.52.4\n"]}]},{"cell_type":"code","source":["from transformers import TrainingArguments\n"],"metadata":{"id":"NB_4B7NwYQD6","executionInfo":{"status":"ok","timestamp":1750963255149,"user_tz":-180,"elapsed":76,"user":{"displayName":"ImEl","userId":"00735694531887898070"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from transformers import DataCollatorForTokenClassification\n","\n","def tokenize_and_align_labels(examples):\n","    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, padding=True)\n","    labels = []\n","    for i, label in enumerate(examples[\"labels\"]):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)\n","        label_ids = []\n","        previous_word_idx = None\n","        for word_idx in word_ids:\n","            if word_idx is None:\n","                label_ids.append(-100)\n","            elif word_idx != previous_word_idx:\n","                label_ids.append(label[word_idx])\n","            else:\n","                label_ids.append(label[word_idx])\n","            previous_word_idx = word_idx\n","        labels.append(label_ids)\n","    tokenized_inputs[\"labels\"] = labels\n","    return tokenized_inputs\n","\n","tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n","data_collator = DataCollatorForTokenClassification(tokenizer)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["7292d9099d0340edae3a78d3b40a22f8","069433a85e0046e198e159c1722bf5f1","11c35adfc5d242a6b127782bafaeb3c7","029e84deb8bf42a9b4d6f14eccf6407e","03c5d5f0fb1a4c27966f87799670edee","cfcc1be95ef4437daa79f7be4b9a3f59","2202e8504755480eb03658e5e1c0a1a4","879d50e59b8f45eb8b63561c835cb980","f78c7da06d284d859bea4e4fefb30b06","d5b48882f2bf405987bf834b2580809a","69efa8558cd44ee9b51c5aeb754c791b","7c9e448f6802407dbe13234ac7c9bab7","707d95fac8df4c7599821840e6f25b24","ef376d98e28d48a395ad40898f196bb5","470550f583bc44469f357b63008651f7","cc1d977c0c0f4a5aa96661f419966d90","34ac351d9cbb44f4990f8cf1c4517c3d","39adb4dabd8646edbd39f02668c13833","15540f6473c149dd8b33d9704dd715a8","3cac139250724422b6a6f5dcadcb9caa","2cfe49185366491893457305810662ac","0dcedba7fcaf478184dae6bf84dc99a3"]},"id":"HqjctLrq8JI-","executionInfo":{"status":"ok","timestamp":1750963258875,"user_tz":-180,"elapsed":341,"user":{"displayName":"ImEl","userId":"00735694531887898070"}},"outputId":"a7ebba23-7fd8-4bb1-ff29-5a377e320d29"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/8 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7292d9099d0340edae3a78d3b40a22f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/2 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c9e448f6802407dbe13234ac7c9bab7"}},"metadata":{}}]},{"cell_type":"code","source":["print(label_list)\n","print(f\"# Labesls in dataset: {len(label_list)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b_zp6cv2Es5s","executionInfo":{"status":"ok","timestamp":1750963266875,"user_tz":-180,"elapsed":252,"user":{"displayName":"ImEl","userId":"00735694531887898070"}},"outputId":"185ec6cd-d491-44af-c069-e8bf39843eae"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["['B-CONTACT', 'B-LOC', 'B-PRICE', 'B-Product', 'I-LOC', 'I-Product', 'O']\n","# Labesls in dataset: 7\n"]}]},{"cell_type":"markdown","source":["##Initialize Model and Trainer\n","\n","The pretrained model you loaded ```(Davlan/xlm-roberta-base-ner-hrl) ```has 9 NER labels.\n","so I've included ```ignore_mismatched_sizes=True ```"],"metadata":{"id":"fEOo5iE2AYOu"}},{"cell_type":"code","source":["from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n","\n","# 1. Load base model (not pre-trained NER)\n","model_checkpoint = \"xlm-roberta-base\"\n","model = AutoModelForTokenClassification.from_pretrained(\n","    model_checkpoint,\n","    num_labels=len(label_list),\n","    id2label=id_to_label,\n","    label2id=label_to_id\n",")\n","\n","# 2. Updated training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"results\",\n","    eval_strategy=\"epoch\",  # or \"steps\" with eval_steps=500\n","    learning_rate=2e-5,\n","    warmup_steps=500,\n","    per_device_train_batch_size=8,\n","    num_train_epochs=5,\n","    weight_decay=0.01,\n","    logging_steps=10,  # Log loss every 10 steps\n","    log_level=\"info\",\n",")\n","\n","# 3. Initialize Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator\n",")\n","\n","# 4. Train\n","trainer.train()\n","\n","\n","eval_results = trainer.evaluate()\n","print(f\"Evaluation Results: {eval_results}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"_Y2b-sa__yyU","executionInfo":{"status":"ok","timestamp":1750966514120,"user_tz":-180,"elapsed":174697,"user":{"displayName":"ImEl","userId":"00735694531887898070"}},"outputId":"394cc928-30f5-4235-92f0-024dccaff24c"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/config.json\n","Model config XLMRobertaConfig {\n","  \"architectures\": [\n","    \"XLMRobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"B-CONTACT\",\n","    \"1\": \"B-LOC\",\n","    \"2\": \"B-PRICE\",\n","    \"3\": \"B-Product\",\n","    \"4\": \"I-LOC\",\n","    \"5\": \"I-Product\",\n","    \"6\": \"O\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"B-CONTACT\": 0,\n","    \"B-LOC\": 1,\n","    \"B-PRICE\": 2,\n","    \"B-Product\": 3,\n","    \"I-LOC\": 4,\n","    \"I-Product\": 5,\n","    \"O\": 6\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/model.safetensors\n","Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/tmp/ipython-input-18-1917843838.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","The following columns in the Training set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","***** Running training *****\n","  Num examples = 8\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 5\n","  Number of trainable parameters = 277,458,439\n","Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5/5 02:22, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>1.740428</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>1.740213</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>1.739782</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>1.739135</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>No log</td>\n","      <td>1.738269</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","Saving model checkpoint to results/checkpoint-5\n","Configuration saved in results/checkpoint-5/config.json\n","Model weights saved in results/checkpoint-5/model.safetensors\n","tokenizer config file saved in results/checkpoint-5/tokenizer_config.json\n","Special tokens file saved in results/checkpoint-5/special_tokens_map.json\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1/1 : < :]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Evaluation Results: {'eval_loss': 1.738269329071045, 'eval_runtime': 0.6279, 'eval_samples_per_second': 3.185, 'eval_steps_per_second': 1.593, 'epoch': 5.0}\n"]}]},{"cell_type":"markdown","source":["##Add Metrics Computation"],"metadata":{"id":"PYu55rcttJr7"}},{"cell_type":"code","source":["from seqeval.metrics import f1_score, precision_score, recall_score\n","import numpy as np\n","\n","def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    # Remove ignored index (special tokens)\n","    true_predictions = [\n","        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    return {\n","        \"precision\": precision_score(true_labels, true_predictions),\n","        \"recall\": recall_score(true_labels, true_predictions),\n","        \"f1\": f1_score(true_labels, true_predictions),\n","    }"],"metadata":{"id":"aMamd2nuEp-s","executionInfo":{"status":"ok","timestamp":1750963673118,"user_tz":-180,"elapsed":53,"user":{"displayName":"ImEl","userId":"00735694531887898070"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"LpUZZjNcw4nW"}},{"cell_type":"code","source":["from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n","\n","# 1. Load base model (not pre-trained NER)\n","model_checkpoint = \"xlm-roberta-base\"\n","model = AutoModelForTokenClassification.from_pretrained(\n","    model_checkpoint,\n","    num_labels=len(label_list),\n","    id2label=id_to_label,\n","    label2id=label_to_id\n",")\n","\n","# 2. Updated training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"results\",\n","    eval_strategy=\"epoch\",  # or \"steps\" with eval_steps=500\n","    learning_rate=2e-5,\n","    warmup_steps=500,\n","    per_device_train_batch_size=8,\n","    num_train_epochs=7,\n","    weight_decay=0.01,\n","    logging_steps=10,  # Log loss every 10 steps\n","    log_level=\"info\",\n",")\n","\n","# 3. Initialize Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"test\"],\n","    compute_metrics=compute_metrics, #this one is added\n","    tokenizer=tokenizer,\n","    data_collator=data_collator\n",")\n","\n","# 4. Train\n","trainer.train()\n","\n","\n","eval_results = trainer.evaluate()\n","print(f\"Evaluation Results: {eval_results}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"EWWJQ0WXw6gS","executionInfo":{"status":"ok","timestamp":1750966745017,"user_tz":-180,"elapsed":178729,"user":{"displayName":"ImEl","userId":"00735694531887898070"}},"outputId":"da8fa5ee-3a5d-4493-f8ab-6dc4f517f8e0"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/config.json\n","Model config XLMRobertaConfig {\n","  \"architectures\": [\n","    \"XLMRobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"B-CONTACT\",\n","    \"1\": \"B-LOC\",\n","    \"2\": \"B-PRICE\",\n","    \"3\": \"B-Product\",\n","    \"4\": \"I-LOC\",\n","    \"5\": \"I-Product\",\n","    \"6\": \"O\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"B-CONTACT\": 0,\n","    \"B-LOC\": 1,\n","    \"B-PRICE\": 2,\n","    \"B-Product\": 3,\n","    \"I-LOC\": 4,\n","    \"I-Product\": 5,\n","    \"O\": 6\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/model.safetensors\n","Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/tmp/ipython-input-19-1285494237.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","The following columns in the Training set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","***** Running training *****\n","  Num examples = 8\n","  Num Epochs = 7\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 7\n","  Number of trainable parameters = 277,458,439\n","Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [7/7 02:32, Epoch 7/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>2.208729</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.066667</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>2.208469</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.066667</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>2.207953</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.066667</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>2.207182</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.066667</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>No log</td>\n","      <td>2.206150</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.066667</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>No log</td>\n","      <td>2.204859</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.066667</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>No log</td>\n","      <td>2.203317</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.066667</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","Saving model checkpoint to results/checkpoint-7\n","Configuration saved in results/checkpoint-7/config.json\n","Model weights saved in results/checkpoint-7/model.safetensors\n","tokenizer config file saved in results/checkpoint-7/tokenizer_config.json\n","Special tokens file saved in results/checkpoint-7/special_tokens_map.json\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1/1 : < :]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Evaluation Results: {'eval_loss': 2.2033169269561768, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.06666666666666667, 'eval_runtime': 0.4239, 'eval_samples_per_second': 4.719, 'eval_steps_per_second': 2.359, 'epoch': 7.0}\n"]}]},{"cell_type":"code","source":["from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n","\n","# 1. Load base model (not pre-trained NER)\n","model_checkpoint = \"xlm-roberta-base\"\n","model = AutoModelForTokenClassification.from_pretrained(\n","    model_checkpoint,\n","    num_labels=len(label_list),\n","    id2label=id_to_label,\n","    label2id=label_to_id\n",")\n","\n","# 2. Updated training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"/content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/results\",\n","    eval_strategy=\"epoch\",  # or \"steps\" with eval_steps=500\n","    learning_rate=2e-5,\n","    warmup_steps=500,\n","    per_device_train_batch_size=8,\n","    num_train_epochs=8,\n","    weight_decay=0.01,\n","    logging_steps=10,  # Log loss every 10 steps\n","    log_level=\"info\",\n",")\n","\n","# 3. Initialize Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"test\"],\n","    compute_metrics=compute_metrics, #this one is added\n","    tokenizer=tokenizer,\n","    data_collator=data_collator\n",")\n","\n","# 4. Train\n","trainer.train()\n","\n","\n","eval_results = trainer.evaluate()\n","print(f\"Evaluation Results: {eval_results}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"-BF8PgRNlh_A","executionInfo":{"status":"ok","timestamp":1750964196916,"user_tz":-180,"elapsed":180896,"user":{"displayName":"ImEl","userId":"00735694531887898070"}},"outputId":"382c9d19-dfa7-436c-8ae0-e005b781be44"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/config.json\n","Model config XLMRobertaConfig {\n","  \"architectures\": [\n","    \"XLMRobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"B-CONTACT\",\n","    \"1\": \"B-LOC\",\n","    \"2\": \"B-PRICE\",\n","    \"3\": \"B-Product\",\n","    \"4\": \"I-LOC\",\n","    \"5\": \"I-Product\",\n","    \"6\": \"O\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"B-CONTACT\": 0,\n","    \"B-LOC\": 1,\n","    \"B-PRICE\": 2,\n","    \"B-Product\": 3,\n","    \"I-LOC\": 4,\n","    \"I-Product\": 5,\n","    \"O\": 6\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/model.safetensors\n","Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/tmp/ipython-input-14-2402940796.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","The following columns in the Training set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","***** Running training *****\n","  Num examples = 8\n","  Num Epochs = 8\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 8\n","  Number of trainable parameters = 277,458,439\n","Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [8/8 02:36, Epoch 8/8]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>1.667598</td>\n","      <td>0.029412</td>\n","      <td>0.090909</td>\n","      <td>0.044444</td>\n","      <td>0.522222</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>1.667360</td>\n","      <td>0.029412</td>\n","      <td>0.090909</td>\n","      <td>0.044444</td>\n","      <td>0.522222</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>1.666888</td>\n","      <td>0.029412</td>\n","      <td>0.090909</td>\n","      <td>0.044444</td>\n","      <td>0.522222</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>1.666182</td>\n","      <td>0.029412</td>\n","      <td>0.090909</td>\n","      <td>0.044444</td>\n","      <td>0.522222</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>No log</td>\n","      <td>1.665239</td>\n","      <td>0.029412</td>\n","      <td>0.090909</td>\n","      <td>0.044444</td>\n","      <td>0.522222</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>No log</td>\n","      <td>1.664061</td>\n","      <td>0.030303</td>\n","      <td>0.090909</td>\n","      <td>0.045455</td>\n","      <td>0.511111</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>No log</td>\n","      <td>1.662650</td>\n","      <td>0.030303</td>\n","      <td>0.090909</td>\n","      <td>0.045455</td>\n","      <td>0.511111</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>No log</td>\n","      <td>1.661006</td>\n","      <td>0.033333</td>\n","      <td>0.090909</td>\n","      <td>0.048780</td>\n","      <td>0.533333</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/results/checkpoint-8\n","Configuration saved in /content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/results/checkpoint-8/config.json\n","Model weights saved in /content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/results/checkpoint-8/model.safetensors\n","tokenizer config file saved in /content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/results/checkpoint-8/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/results/checkpoint-8/special_tokens_map.json\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1/1 : < :]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Evaluation Results: {'eval_loss': 1.661006212234497, 'eval_precision': 0.03333333333333333, 'eval_recall': 0.09090909090909091, 'eval_f1': 0.04878048780487805, 'eval_accuracy': 0.5333333333333333, 'eval_runtime': 0.7351, 'eval_samples_per_second': 2.721, 'eval_steps_per_second': 1.36, 'epoch': 8.0}\n"]}]},{"cell_type":"code","source":["from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n","\n","# 1. Load base model (not pre-trained NER)\n","model_checkpoint = \"xlm-roberta-base\"\n","model = AutoModelForTokenClassification.from_pretrained(\n","    model_checkpoint,\n","    num_labels=len(label_list),\n","    id2label=id_to_label,\n","    label2id=label_to_id\n",")\n","\n","# 2. Updated training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"/content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/results\",\n","    eval_strategy=\"epoch\",  # or \"steps\" with eval_steps=500\n","    learning_rate=2e-5,\n","    warmup_steps=500,\n","    per_device_train_batch_size=8,\n","    num_train_epochs=9,\n","    weight_decay=0.01,\n","    logging_steps=10,  # Log loss every 10 steps\n","    log_level=\"info\",\n",")\n","\n","# 3. Initialize Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"test\"],\n","    compute_metrics=compute_metrics, #this one is added\n","    tokenizer=tokenizer,\n","    data_collator=data_collator\n",")\n","\n","# 4. Train\n","trainer.train()\n","\n","\n","eval_results = trainer.evaluate()\n","print(f\"Evaluation Results: {eval_results}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"sKJ8hhLOmHii","executionInfo":{"status":"ok","timestamp":1750964483479,"user_tz":-180,"elapsed":219801,"user":{"displayName":"ImEl","userId":"00735694531887898070"}},"outputId":"2cfe6aee-6438-4b3b-818b-fd8a9dae9a99"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/config.json\n","Model config XLMRobertaConfig {\n","  \"architectures\": [\n","    \"XLMRobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"B-CONTACT\",\n","    \"1\": \"B-LOC\",\n","    \"2\": \"B-PRICE\",\n","    \"3\": \"B-Product\",\n","    \"4\": \"I-LOC\",\n","    \"5\": \"I-Product\",\n","    \"6\": \"O\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"B-CONTACT\": 0,\n","    \"B-LOC\": 1,\n","    \"B-PRICE\": 2,\n","    \"B-Product\": 3,\n","    \"I-LOC\": 4,\n","    \"I-Product\": 5,\n","    \"O\": 6\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/model.safetensors\n","Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/tmp/ipython-input-15-1905241292.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","The following columns in the Training set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","***** Running training *****\n","  Num examples = 8\n","  Num Epochs = 9\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 9\n","  Number of trainable parameters = 277,458,439\n","Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [9/9 03:14, Epoch 9/9]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>1.535733</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.811111</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>1.535522</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.811111</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>1.535101</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.811111</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>1.534468</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.811111</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>No log</td>\n","      <td>1.533626</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.811111</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>No log</td>\n","      <td>1.532574</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.811111</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>No log</td>\n","      <td>1.531317</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.811111</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>No log</td>\n","      <td>1.529844</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.811111</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>No log</td>\n","      <td>1.528177</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.811111</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/results/checkpoint-9\n","Configuration saved in /content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/results/checkpoint-9/config.json\n","Model weights saved in /content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/results/checkpoint-9/model.safetensors\n","tokenizer config file saved in /content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/results/checkpoint-9/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/results/checkpoint-9/special_tokens_map.json\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1/1 : < :]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Evaluation Results: {'eval_loss': 1.5281773805618286, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.8111111111111111, 'eval_runtime': 0.5648, 'eval_samples_per_second': 3.541, 'eval_steps_per_second': 1.77, 'epoch': 9.0}\n"]}]},{"cell_type":"code","source":["from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n","\n","# 1. Load base model (not pre-trained NER)\n","model_checkpoint = \"xlm-roberta-base\"\n","model = AutoModelForTokenClassification.from_pretrained(\n","    model_checkpoint,\n","    num_labels=len(label_list),\n","    id2label=id_to_label,\n","    label2id=label_to_id\n",")\n","\n","# 2. Updated training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"/content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/results\",\n","    eval_strategy=\"epoch\",  # or \"steps\" with eval_steps=500\n","    learning_rate=2e-5,\n","    warmup_steps=500,\n","    per_device_train_batch_size=8,\n","    num_train_epochs=10,\n","    weight_decay=0.01,\n","    logging_steps=10,  # Log loss every 10 steps\n","    log_level=\"info\",\n",")\n","\n","# 3. Initialize Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"test\"],\n","    compute_metrics=compute_metrics, #this one is added\n","    tokenizer=tokenizer,\n","    data_collator=data_collator\n",")\n","\n","# 4. Train\n","trainer.train()\n","\n","\n","eval_results = trainer.evaluate()\n","print(f\"Evaluation Results: {eval_results}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"9D4CIaWzn23I","executionInfo":{"status":"ok","timestamp":1750965238376,"user_tz":-180,"elapsed":220081,"user":{"displayName":"ImEl","userId":"00735694531887898070"}},"outputId":"25a1e0d4-0bb1-468e-a1c7-e8cf5ad1c829"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/config.json\n","Model config XLMRobertaConfig {\n","  \"architectures\": [\n","    \"XLMRobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"B-CONTACT\",\n","    \"1\": \"B-LOC\",\n","    \"2\": \"B-PRICE\",\n","    \"3\": \"B-Product\",\n","    \"4\": \"I-LOC\",\n","    \"5\": \"I-Product\",\n","    \"6\": \"O\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"B-CONTACT\": 0,\n","    \"B-LOC\": 1,\n","    \"B-PRICE\": 2,\n","    \"B-Product\": 3,\n","    \"I-LOC\": 4,\n","    \"I-Product\": 5,\n","    \"O\": 6\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/model.safetensors\n","Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/tmp/ipython-input-17-135577595.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","The following columns in the Training set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","***** Running training *****\n","  Num examples = 8\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 10\n","  Number of trainable parameters = 277,458,439\n","Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [10/10 02:59, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>1.785783</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>1.785558</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>1.785108</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>1.784435</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>No log</td>\n","      <td>1.783535</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>No log</td>\n","      <td>1.782408</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>No log</td>\n","      <td>1.781057</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>No log</td>\n","      <td>1.779474</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>No log</td>\n","      <td>1.777681</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.355556</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>1.811200</td>\n","      <td>1.775658</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.377778</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/results/checkpoint-10\n","Configuration saved in /content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/results/checkpoint-10/config.json\n","Model weights saved in /content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/results/checkpoint-10/model.safetensors\n","tokenizer config file saved in /content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/results/checkpoint-10/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/results/checkpoint-10/special_tokens_map.json\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n","/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1/1 : < :]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Evaluation Results: {'eval_loss': 1.775658130645752, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.37777777777777777, 'eval_runtime': 0.7481, 'eval_samples_per_second': 2.673, 'eval_steps_per_second': 1.337, 'epoch': 10.0}\n"]}]},{"cell_type":"markdown","source":["##Evaluate"],"metadata":{"id":"wtBXkWt_ya6O"}},{"cell_type":"code","source":["from datasets import load_metric\n","\n","metric = load_metric(\"seqeval\")\n","\n","def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = predictions.argmax(axis=-1)\n","\n","    true_predictions = [\n","        [id_to_label[p] for (p, l) in zip(pred, label) if l != -100]\n","        for pred, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [id_to_label[l] for (p, l) in zip(pred, label) if l != -100]\n","        for pred, label in zip(predictions, labels)\n","    ]\n","    results = metric.compute(predictions=true_predictions, references=true_labels)\n","    return {\n","        \"precision\": results[\"overall_precision\"],\n","        \"recall\": results[\"overall_recall\"],\n","        \"f1\": results[\"overall_f1\"],\n","        \"accuracy\": results[\"overall_accuracy\"],\n","    }\n","\n","trainer.compute_metrics = compute_metrics\n","trainer.evaluate()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":369},"id":"Q3B2PN2iyW3b","executionInfo":{"status":"ok","timestamp":1750964543452,"user_tz":-180,"elapsed":1148,"user":{"displayName":"ImEl","userId":"00735694531887898070"}},"outputId":"0548d0b2-f3e7-476f-924d-46b04d46ba99"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1/1 00:59]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"execute_result","data":{"text/plain":["{'eval_loss': 1.5281773805618286,\n"," 'eval_precision': 0.0,\n"," 'eval_recall': 0.0,\n"," 'eval_f1': 0.0,\n"," 'eval_accuracy': 0.8111111111111111,\n"," 'eval_runtime': 0.4656,\n"," 'eval_samples_per_second': 4.295,\n"," 'eval_steps_per_second': 2.148,\n"," 'epoch': 9.0}"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["##Save the model"],"metadata":{"id":"fCWwPgIfB8mA"}},{"cell_type":"code","source":["trainer.save__model(\"/content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/model\")"],"metadata":{"id":"nI9xwoz6B7rk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Task 5\n","## Inference and Deployment"],"metadata":{"id":"IIT12vqvE7GS"}},{"cell_type":"code","source":["!pip install shap lime transformers sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rg_CZvAlGdFN","executionInfo":{"status":"ok","timestamp":1750966907059,"user_tz":-180,"elapsed":14708,"user":{"displayName":"ImEl","userId":"00735694531887898070"}},"outputId":"e75a5fc3-dab7-480f-da64-32e5641be5cd"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.48.0)\n","Collecting lime\n","  Downloading lime-0.2.0.1.tar.gz (275 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from shap) (2.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from shap) (1.15.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from shap) (1.6.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from shap) (2.2.2)\n","Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n","Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (24.2)\n","Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n","Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from shap) (4.14.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from lime) (3.10.0)\n","Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.11/dist-packages (from lime) (0.25.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap) (0.43.0)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (3.5)\n","Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (11.2.1)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2.37.0)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2025.6.11)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (0.4)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (3.6.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (4.58.4)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.4.8)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n","Building wheels for collected packages: lime\n","  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=e5e47231a23efc67d362b7be27e718bc666bd030a0d06fa4b12ae84a1e54cc8a\n","  Stored in directory: /root/.cache/pip/wheels/85/fa/a3/9c2d44c9f3cd77cf4e533b58900b2bf4487f2a17e8ec212a3d\n","Successfully built lime\n","Installing collected packages: lime\n","Successfully installed lime-0.2.0.1\n"]}]},{"cell_type":"markdown","source":["##Loading Trained Model and Tokenizer"],"metadata":{"id":"Hrpl42q6FLL3"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n","\n","model_path = \"/content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/results/checkpoint-7\"\n","\n","ner_pipeline = pipeline(\"ner\", model=model_path, tokenizer=model_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"powrdkY2E0sT","executionInfo":{"status":"ok","timestamp":1750888935710,"user_tz":-180,"elapsed":2330,"user":{"displayName":"ImEl","userId":"00735694531887898070"}},"outputId":"bc0d0292-c2f9-4b23-ff7c-e822010fad1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file /content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/results/checkpoint-7/config.json\n","Model config XLMRobertaConfig {\n","  \"architectures\": [\n","    \"XLMRobertaForTokenClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"B-CONTACT\",\n","    \"1\": \"B-LOC\",\n","    \"2\": \"B-PRICE\",\n","    \"3\": \"B-Product\",\n","    \"4\": \"I-LOC\",\n","    \"5\": \"I-Product\",\n","    \"6\": \"O\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"B-CONTACT\": 0,\n","    \"B-LOC\": 1,\n","    \"B-PRICE\": 2,\n","    \"B-Product\": 3,\n","    \"I-LOC\": 4,\n","    \"I-Product\": 5,\n","    \"O\": 6\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading configuration file /content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/results/checkpoint-7/config.json\n","Model config XLMRobertaConfig {\n","  \"architectures\": [\n","    \"XLMRobertaForTokenClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"B-CONTACT\",\n","    \"1\": \"B-LOC\",\n","    \"2\": \"B-PRICE\",\n","    \"3\": \"B-Product\",\n","    \"4\": \"I-LOC\",\n","    \"5\": \"I-Product\",\n","    \"6\": \"O\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"B-CONTACT\": 0,\n","    \"B-LOC\": 1,\n","    \"B-PRICE\": 2,\n","    \"B-Product\": 3,\n","    \"I-LOC\": 4,\n","    \"I-Product\": 5,\n","    \"O\": 6\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file /content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/results/checkpoint-7/model.safetensors\n","All model checkpoint weights were used when initializing XLMRobertaForTokenClassification.\n","\n","All the weights of XLMRobertaForTokenClassification were initialized from the model checkpoint at /content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/results/checkpoint-7.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForTokenClassification for predictions without further training.\n","loading file sentencepiece.bpe.model\n","loading file tokenizer.json\n","loading file added_tokens.json\n","loading file special_tokens_map.json\n","loading file tokenizer_config.json\n","loading file chat_template.jinja\n","Device set to use cpu\n"]}]},{"cell_type":"code","source":["import torch"],"metadata":{"id":"NFwMkiP9JTjc","executionInfo":{"status":"ok","timestamp":1750966954979,"user_tz":-180,"elapsed":8590,"user":{"displayName":"ImEl","userId":"00735694531887898070"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["##SHAP Analysis (Global Explanations)"],"metadata":{"id":"FBDZuK9aHSj3"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n","\n","model_path = \"/content/drive/MyDrive/10Acadamy/amharic_e-commerce_data_extractor/results/checkpoint-7\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","model = AutoModelForTokenClassification.from_pretrained(model_path)\n","\n","ner_pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jwpeh5ovQbU0","executionInfo":{"status":"ok","timestamp":1750967191888,"user_tz":-180,"elapsed":1198,"user":{"displayName":"ImEl","userId":"00735694531887898070"}},"outputId":"e927b381-d443-456e-8226-6b60f0a9ae1e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cpu\n"]}]},{"cell_type":"code","source":["sample_text = \"CHEKICH በ2800 ብር በአዲስ አበባ\"\n","\n","entities = ner_pipe(sample_text)\n","for entity in entities:\n","    print(f\"{entity['word']} — {entity['entity_group']} — Score: {entity['score']:.2f}\")\n"],"metadata":{"id":"Rv_jdh1wQsJP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750967205799,"user_tz":-180,"elapsed":596,"user":{"displayName":"ImEl","userId":"00735694531887898070"}},"outputId":"63f95825-7da6-477d-d8a8-d9d8ae48b26c"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["CHE — LOC — Score: 0.21\n","በ — LOC — Score: 0.22\n","2 — LOC — Score: 0.23\n","በአዲስ — LOC — Score: 0.22\n"]}]},{"cell_type":"code","source":["from transformers import pipeline\n","\n","# 1. Load pipeline with aggregation strategy\n","ner_pipe = pipeline(\n","    \"ner\",\n","    model=model_path,\n","    tokenizer=model_path,\n","    aggregation_strategy=\"simple\"  # Groups subwords\n",")\n","\n","# 2. Process text with word boundaries\n","sample_text = \"አዲስ ሞዴል ቤት ማሽን በ 23000 ብር በአዲስ አበባ ሽያጭ ላይ ነው።\"\n","entities = ner_pipe(sample_text)\n","\n","# 3. Print clean results\n","for entity in entities:\n","    print(f\"Entity: {entity['word']}\")\n","    print(f\"Type: {entity['entity_group']}\")\n","    print(f\"Confidence: {entity['score']:.2f}\")\n","    print(f\"Position: {entity['start']}-{entity['end']}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8DnVDfhHV_dA","executionInfo":{"status":"ok","timestamp":1750967217280,"user_tz":-180,"elapsed":1637,"user":{"displayName":"ImEl","userId":"00735694531887898070"}},"outputId":"f3cd5d85-6c5e-479a-cbd0-25847261ed5b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cpu\n"]},{"output_type":"stream","name":"stdout","text":["Entity: ሞ\n","Type: LOC\n","Confidence: 0.22\n","Position: 4-5\n","\n","Entity: ል\n","Type: LOC\n","Confidence: 0.21\n","Position: 6-7\n","\n","Entity: ማ\n","Type: LOC\n","Confidence: 0.22\n","Position: 11-12\n","\n","Entity: በ\n","Type: LOC\n","Confidence: 0.22\n","Position: 15-16\n","\n","Entity: 2\n","Type: LOC\n","Confidence: 0.23\n","Position: 17-18\n","\n","Entity: ብር\n","Type: LOC\n","Confidence: 0.22\n","Position: 23-25\n","\n","Entity: በአዲስ\n","Type: LOC\n","Confidence: 0.22\n","Position: 26-30\n","\n","Entity: ያ\n","Type: LOC\n","Confidence: 0.22\n","Position: 36-37\n","\n","Entity: ጭ\n","Type: LOC\n","Confidence: 0.21\n","Position: 37-38\n","\n","Entity: ላይ\n","Type: LOC\n","Confidence: 0.21\n","Position: 39-41\n","\n","Entity: ነው።\n","Type: LOC\n","Confidence: 0.24\n","Position: 42-45\n","\n"]}]},{"cell_type":"code","source":["texts = [\n","    \"አዲስ ሞዴል ቤት ማሽን በ 23000 ብር በአዲስ አበባ ሽያጭ ላይ ነው።\",\n","    \"የህጻናት ቀሚስ በ1500 ብር አቅምን ላይ ነው።\",\n","    \"አስደናቂ ሻምፓን ከCHEKICH በ2800 ብር\"\n","]\n"],"metadata":{"id":"LP-uLR4ozM2_","executionInfo":{"status":"ok","timestamp":1750967574873,"user_tz":-180,"elapsed":17,"user":{"displayName":"ImEl","userId":"00735694531887898070"}}},"execution_count":10,"outputs":[]}]}